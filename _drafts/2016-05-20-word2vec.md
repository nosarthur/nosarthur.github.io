---
layout: post
title:  Words to vectors
date:   2016-05-20 20:00:00 -0500
categories: [machine learning]
comments: true
tags: [machine learning]
---

Numbers are easier to dealt with than texts for computers. 
A good numerical representation for words is thus of great importance for further linguistic analysis. 
To make discussion concrete, let us assume that our text contains N words with V vocabularies. 
Note N>V due to repeated words. 
For practical applications, both N and V are large (think of V as thousands and N as millions). 
According to [Wikipedia][wiki], a native english speaker knows about 10,000 words and mastering 3000 words is necessary for a non-native speaker. 
According to [Steven Pinker][Pinker], a typical high school graduate has a vocabulary of 60,000 words.

The most straightforward word representation is the so-called one-hot representation. Here all words live in the V dimensional space and every one of them is a unit vector along the axes. These one-hot vectors are not particularly useful since all semantic information is eliminated. The only information left is that there are V vocabularies.

A better representation built on the one-hot vectors is the distributional vector representation. Here the idea is that a word is characterized by the words near it, just like a person is somewhat characterized by his or her friends. Then one can define a window as a word's neighborhood and do statistics to see how likely each vocabulary is going to show up in the neighborhood. In the end, the distribution vector of a vocabulary contains all the probabilities that each vocabulary is contained in its neighborhood. The advantage of the distributional representation is that now similarity between vocabularies is encoded in the closeness between the distributional vectors. It is still far from great though. For example, word order is completely eliminated. 

The more recent trend is called word embedding. Here  instead of V dimensional vectors, lower dimensional vectors are used (think of a few hundred). 

semantic similarity
analogy
The analogy is encoded in the difference between vectors. The famous example is king-man+woman=queen.

paragraph vector

### Resources:
* RADIM ŘEHŮŘEK's word2vec [tutorial][tutorial] 
* gensim word2vec [page][gensim]
* python nltk [book][book]

### Notes:
* To use word2vec in gensim, make sure Cython is installed. It helps with speed.

[tutorial]: http://rare-technologies.com/word2vec-tutorial/
[gensim]: https://radimrehurek.com/gensim/models/word2vec.html
[book]: http://www.nltk.org/book/
[wiki]: https://en.wikipedia.org/wiki/Vocabulary#Native-language_vocabulary_size
[Pinker]: https://www.youtube.com/watch?v=Q-B_ONJIEcE
